{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc96HGV0pYkJ"
   },
   "source": [
    "# Introduction to Data Science 2025\n",
    "\n",
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWh-qfRJpYkK"
   },
   "source": [
    "## Exercise 1 | Titanic: data preprocessing and imputation\n",
    "<span style=\"font-weight: bold\"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZy31vJxpYkK"
   },
   "source": [
    "Download the [Titanic dataset](https://www.kaggle.com/c/titanic) [train.csv] from Kaggle or <span style=\"font-weight: 500\">directly from the course material</span>, and complete the following exercises. If you choose to download the dataset from Kaggle, you will need to create a Kaggle account unless you already have one, but it is quite straightforward.\n",
    "\n",
    "The dataset consists of personal information of all the passengers on board the RMS Titanic, along with information about whether they survived the iceberg collision or not.\n",
    "\n",
    "1. Your first task is to read the data file and print the shape of the data.\n",
    "\n",
    "    <span style=\"font-weight: 500\"> *Hint 1: You can read them into a Pandas dataframe if you wish.*</span>\n",
    "    \n",
    "    <span style=\"font-weight: 500\"> *Hint 2: The shape of the data should be (891, 12).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "l4kCNDespYkK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data frame: (891, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assumes the source data as a csv file within the data folder next to the notebook file\n",
    "df = pd.read_csv(\"./data/titanic.csv\")\n",
    "print(\"Shape of data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCZoOS-wpYkL"
   },
   "source": [
    "2. Let's look at the data and get started with some preprocessing. Some of the columns, e.g <span style=\"font-weight: 500\"> *Name*</span>, simply identify a person and are not useful for prediction tasks. Try to identify these columns, and remove them.\n",
    "\n",
    "    <span style=\"font-weight: 500\"> *Hint: The shape of the data should now be (891, 9).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "m18V-jpTpYkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data frame: (891, 9)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"Name\", \"Ticket\", \"Embarked\"])\n",
    "print(\"Shape of data frame:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48AcPDOrpYkL"
   },
   "source": [
    "3. The column <span style=\"font-weight: 500\">*Cabin*</span> contains a letter and a number. A smart catch at this point would be to notice that the letter stands for the deck level on the ship. Keeping just the deck information would be more informative when developing, e.g. a classifier that predicts whether a passenger survived. The next step in our preprocessing will be to add a new column to the dataset, which consists simply of the deck letter. You can then remove the original <span style=\"font-weight: 500\">*Cabin*</span>-column.\n",
    "\n",
    "<span style=\"font-weight: 500\">*Hint: The deck letters should be ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'T'].*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "OQX5LmWrpYkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deck column of first few rows:\n",
      "0    NaN\n",
      "1      C\n",
      "2    NaN\n",
      "3      C\n",
      "4    NaN\n",
      "Name: Deck, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = df.assign(Deck=df[\"Cabin\"].str[0])\n",
    "df = df.drop(columns=[\"Cabin\"])\n",
    "\n",
    "print(\"Deck column of first few rows:\")\n",
    "print(df[\"Deck\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mupSH5tXpYkL"
   },
   "source": [
    "4. You’ll notice that some of the columns, such as the previously added deck number, are [categorical](https://en.wikipedia.org/wiki/Categorical_variable). To preprocess the categorical variables so that they're ready for further computation, we need to avoid the current string format of the values. This means the next step for each categorical variable is to transform the string values to numeric ones, that correspond to a unique integer ID representative of each distinct category. This process is called label encoding and you can read more about it [here](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint: Pandas can do this for you.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "cfb0_uzRpYkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after converting categorical variables to codes:\n",
      "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Deck\n",
      "0            1         0       3    1  22.0      1      0   7.2500    -1\n",
      "1            2         1       1    0  38.0      1      0  71.2833     2\n",
      "2            3         1       3    0  26.0      0      0   7.9250    -1\n",
      "3            4         1       1    0  35.0      1      0  53.1000     2\n",
      "4            5         0       3    1  35.0      0      0   8.0500    -1\n",
      "\n",
      "Data types after label encoding:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Sex               int8\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Fare           float64\n",
      "Deck              int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = df.assign(Deck=df[\"Deck\"].astype(\"category\"))\n",
    "df = df.assign(Deck=df[\"Deck\"].cat.codes)\n",
    "\n",
    "df = df.assign(Sex=df[\"Sex\"].astype(\"category\"))\n",
    "df = df.assign(Sex=df[\"Sex\"].cat.codes)\n",
    "\n",
    "print(\"Data after converting categorical variables to codes:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nData types after label encoding:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32_hHxGLpYkL"
   },
   "source": [
    "5. Next, let's look into missing value **imputation**. Some of the rows in the data have missing values, e.g when the cabin number of a person is unknown. Most machine learning algorithms have trouble with missing values, and they need to be handled during preprocessing:\n",
    "\n",
    "    a) For continuous variables, replace the missing values with the mean of the non-missing values of that column.\n",
    "\n",
    "    b) For categorical variables, replace the missing values with the mode of the column.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Remember: Even though in the previous step we transformed categorical variables into their numeric representation, they are still categorical.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "e0kh2bbGpYkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Fare             0\n",
      "Deck             0\n",
      "dtype: int64\n",
      "\n",
      "Filled missing Age values with mean: 29.70\n",
      "Deck mode (excluding missing values): 2\n",
      "\n",
      "Dataset shape after imputation: (891, 9)\n",
      "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Deck\n",
      "0            1         0       3    1  22.0      1      0   7.2500     2\n",
      "1            2         1       1    0  38.0      1      0  71.2833     2\n",
      "2            3         1       3    0  26.0      0      0   7.9250     2\n",
      "3            4         1       1    0  35.0      1      0  53.1000     2\n",
      "4            5         0       3    1  35.0      0      0   8.0500     2\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values before imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "age_mean = df[\"Age\"].mean()\n",
    "df[\"Age\"] = df[\"Age\"].fillna(age_mean)\n",
    "\n",
    "print(f\"\\nFilled missing Age values with mean: {age_mean:.2f}\")\n",
    "\n",
    "# -1 represents a missing value, exclude from mode\n",
    "valid_deck_values = df[\"Deck\"][df[\"Deck\"] != -1]\n",
    "\n",
    "if not valid_deck_values.empty:\n",
    "    deck_mode = valid_deck_values.mode()[0]\n",
    "    print(f\"Deck mode (excluding missing values): {deck_mode}\")\n",
    "\n",
    "    df[\"Deck\"] = df[\"Deck\"].replace(-1, deck_mode)\n",
    "else:\n",
    "    print(\"No valid deck values found, keeping -1 as missing indicator\")\n",
    "\n",
    "print(f\"\\nDataset shape after imputation: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15Kbmgx9pYkM"
   },
   "source": [
    "6. At this point, all data is numeric. Write the data, with the modifications we made, to a  <span style=\"font-weight: 500\"> .csv</span> file. Then, write another file, this time in <span style=\"font-weight: 500\">JSON</span> format, with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "J_EhC78NpYkM"
   },
   "outputs": [],
   "source": [
    "#[\n",
    "#    {\n",
    "#        \"Deck\": 0,\n",
    "#        \"Age\": 20,\n",
    "#        \"Survived\", 0\n",
    "#        ...\n",
    "#    },\n",
    "#    {\n",
    "#        ...\n",
    "#    }\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "lxF-ehbapYkM"
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf=\"./titanic_preprocessed.csv\")\n",
    "df.to_json(path_or_buf=\"./titanic_preprocessed.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnkAgzHjpYkM"
   },
   "source": [
    "Study the records and try to see if there is any evident pattern in terms of chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddqs0UqLpYkM"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGnzPePKpYkM"
   },
   "source": [
    "## Exercise 2 | Titanic 2.0: exploratory data analysis\n",
    "\n",
    "In this exercise, we’ll continue to study the Titanic dataset from the last exercise. Now that we have done some preprocessing, it’s time to look at the data with some exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5nTB9ExpYkM"
   },
   "source": [
    "1. First investigate each feature variable in turn. For each categorical variable, find out the mode, i.e., the most frequent value. For numerical variables, calculate the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "hKdMpHZApYkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after preprocessing:\n",
      "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Deck\n",
      "0            1         0       3    1  22.0      1      0   7.2500     2\n",
      "1            2         1       1    0  38.0      1      0  71.2833     2\n",
      "2            3         1       3    0  26.0      0      0   7.9250     2\n",
      "3            4         1       1    0  35.0      1      0  53.1000     2\n",
      "4            5         0       3    1  35.0      0      0   8.0500     2\n",
      "\n",
      "Mode of categorical variables:\n",
      "Survived: 0\n",
      "Pclass: 3\n",
      "Sex: 1\n",
      "Deck: 2\n",
      "\n",
      "Median of numerical variables:\n",
      "Age: 29.70\n",
      "SibSp: 0.00\n",
      "Parch: 0.00\n",
      "Fare: 14.45\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe after preprocessing:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nMode of categorical variables:\")\n",
    "print(f\"Survived: {df[\"Survived\"].mode()[0]}\")\n",
    "print(f\"Pclass: {df[\"Pclass\"].mode()[0]}\")\n",
    "print(f\"Sex: {df[\"Sex\"].mode()[0]}\")\n",
    "print(f\"Deck: {df[\"Deck\"].mode()[0]}\")\n",
    "\n",
    "print(f\"\\nMedian of numerical variables:\")\n",
    "print(f\"Age: {df[\"Age\"].median():.2f}\")\n",
    "print(f\"SibSp: {df[\"SibSp\"].median():.2f}\")\n",
    "print(f\"Parch: {df[\"Parch\"].median():.2f}\")\n",
    "print(f\"Fare: {df[\"Fare\"].median():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjLqfDkpYkM"
   },
   "source": [
    "2. Next, combine the modes of the categorical variables, and the medians of the numerical variables, to construct an imaginary “average survivor”. This \"average survivor\" should represent the typical passenger of the class of passengers who survived. Also following the same principle, construct the “average non-survivor”.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 1: What are the average/most frequent variable values for a non-survivor?*</span>\n",
    "    \n",
    "    <span style=\"font-weight: 500\">*Hint 2: You can split the dataframe in two: one subset containing all the survivors and one consisting of all the non-survivor instances. Then, you can use the summary statistics of each of these dataframe to create a prototype \"average survivor\" and \"average non-survivor\", respectively.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "OhUU2GIDpYkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average survivor:\n",
      "Pclass: 1\n",
      "Sex: 0\n",
      "Deck: 2\n",
      "Age: 29.70\n",
      "SibSp: 0.00\n",
      "Parch: 0.00\n",
      "Fare: 26.00\n",
      "\n",
      "Average non-survivor:\n",
      "Pclass: 3\n",
      "Sex: 1\n",
      "Deck: 2\n",
      "Age: 29.70\n",
      "SibSp: 0.00\n",
      "Parch: 0.00\n",
      "Fare: 10.50\n"
     ]
    }
   ],
   "source": [
    "survivors = df[df[\"Survived\"] == 1]\n",
    "non_survivors = df[df[\"Survived\"] == 0]\n",
    "\n",
    "def print_average(df):\n",
    "    print(f\"Pclass: {df[\"Pclass\"].mode()[0]}\")\n",
    "    print(f\"Sex: {df[\"Sex\"].mode()[0]}\")\n",
    "    print(f\"Deck: {df[\"Deck\"].mode()[0]}\")\n",
    "    print(f\"Age: {df[\"Age\"].median():.2f}\")\n",
    "    print(f\"SibSp: {df[\"SibSp\"].median():.2f}\")\n",
    "    print(f\"Parch: {df[\"Parch\"].median():.2f}\")\n",
    "    print(f\"Fare: {df[\"Fare\"].median():.2f}\")\n",
    "\n",
    "print(f\"Average survivor:\")\n",
    "print_average(survivors)\n",
    "\n",
    "print(f\"\\nAverage non-survivor:\")\n",
    "print_average(non_survivors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3Bax_hlpYkM"
   },
   "source": [
    "3. Next, let's study the distributions of the variables in the two groups (survivor/non-survivor). How well do the average cases represent the respective groups? Can you find actual passengers that are very similar to the (average) representative of their own group? Can you find passengers that are very similar to the (average) representative of the other group?\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Note: Feel free to choose EDA methods according to your preference: non-graphical/graphical, static/interactive - anything goes.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "mNsSMXRMpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc96MC9CpYkM"
   },
   "source": [
    "4. Next, let's continue the analysis by looking into pairwise and multivariate relationships between the variables in the two groups. Try to visualize two variables at a time using, e.g., scatter plots and use a different color to encode the survival status.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 1: You can also check out Seaborn's pairplot function, if you wish.*</span>\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint 2: To better show many data points with the same value for a given variable, you can use either transparency or ‘jitter’.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "fexBqTMQpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNOrCjKzpYkM"
   },
   "source": [
    "5. Finally, recall the preprocessing we did in the first exercise. What can you say about the effect of the choices that were made to use the mode and mean to impute missing values, instead of, for example, ignoring passengers with missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8TeRhWapYkM"
   },
   "source": [
    "*Use this (markdown) cell for your written answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsrhB5wvpYkM"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqgXjyklpYkM"
   },
   "source": [
    "## Exercise 3 | Working with text data 2.0\n",
    "\n",
    "This exercise is related to the second exercise from last week. Find the saved <span style=\"font-weight: 500\">pos.txt</span> and <span style=\"font-weight: 500\">neg.txt</span> files, or, alternatively, you can find the week 1 example solutions on the MOOC platform after Tuesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-DfVlSWpYkM"
   },
   "source": [
    "1. Find the most common words in each file (positive and negative). Examine the results. Do they tend to be general terms relating to the nature of the data? How well do they indicate positive/negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "GDsK6jXMpYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlU_trsrpYkM"
   },
   "source": [
    "2. Compute a [TF/IDF](https://en.wikipedia.org/wiki/Tf–idf) vector for each of the two text files, and make them into a <span style=\"font-weight: 500\">2 x m</span> matrix, where <span style=\"font-weight: 500\">m</span> is the number of unique words in the data. The problem with using the most common words in a review to analyze its contents is that words that are common overall will be common in all reviews (both positive and negative). This means that they probably are not good indicators about the sentiment of a specific review. TF/IDF stands for Term Frequency / Inverse Document Frequency (here the reviews are the documents), and is designed to help by taking into consideration not just the number of times a term occurs (term frequency), but also how many times a word exists in other reviews as well (inverse document frequency). You can use any variant of the formula, as well as off-the-shelf implementations. <span style=\"font-weight: 500\">*Hint: You can use [sklearn](http://scikit-learn.org/).*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "Tt_t-Lx8pYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lsOuLeOpYkM"
   },
   "source": [
    "3. List the words with the highest TF/IDF score in each class (positive | negative), and compare them to the most common words. What do you notice? Did TF/IDF work as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "h1tkcbH5pYkM"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z_vWhnXpYkM"
   },
   "source": [
    "4. Plot the words in each class with their corresponding TF/IDF scores. Note that there will be a lot of words, so you’ll have to think carefully to make your chart clear! If you can’t plot them all, plot a subset – think about how you should choose this subset.\n",
    "\n",
    "    <span style=\"font-weight: 500\">*Hint: you can use word clouds. But feel free to challenge yourselves to think of any other meaningful way to visualize this information!*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "5eUJ3HxlpYkN"
   },
   "outputs": [],
   "source": [
    "# Use this cell for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8-yzV5HpYkN"
   },
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Xd1n98pYkQ"
   },
   "source": [
    "## Exercise 4 | Junk charts\n",
    "\n",
    "There’s a thriving community of chart enthusiasts who keep looking for statistical graphics that they find inappropriate, and which they call “junk charts”, and who often also propose ways to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w78AogNWpYkQ"
   },
   "source": [
    "1. Find at least three statistical visualizations you think are not very good and identify their problems. Copying examples from various junk chart websites is not accepted – you should find your own junk charts, out in the wild. You should be able to find good (or rather, bad) examples quite easily since a significant fraction of charts can have at least *some* issues. The examples you choose should also have different problems, e.g., try to avoid collecting three bar charts, all with problematic axes. Instead, try to find as interesting and diverse examples as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJfbaQH2pYkQ"
   },
   "source": [
    "2. Try to produce improved versions of the charts you selected. The data is of course often not available, but perhaps you can try to extract it, at least approximately, from the chart. Or perhaps you can simulate data that looks similar enough to make the point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-rwa7IrpYkQ"
   },
   "source": [
    "**Submit a PDF with all the charts (the ones you found and the ones you produced).**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
